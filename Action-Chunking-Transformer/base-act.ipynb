{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sledge/miniconda3/envs/deep-learning/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ActionEncoder                                 [1, 128]                  128\n",
       "├─Linear: 1-1                                 [1, 128]                  1,920\n",
       "├─Linear: 1-2                                 [4, 128]                  1,920\n",
       "├─TransformerEncoder: 1-3                     [6, 128]                  --\n",
       "│    └─ModuleList: 2-1                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [6, 128]                  593,024\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [6, 128]                  593,024\n",
       "===============================================================================================\n",
       "Total params: 1,190,016\n",
       "Trainable params: 1,190,016\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 6.33\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.24\n",
       "Params size (MB): 4.23\n",
       "Estimated Total Size (MB): 4.47\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First define the encoder, the encoder is a bert like transformer encoder. It inserts a [CLS] token at the beginning, followed by joint position, followed by k action sequences.\n",
    "\n",
    "class ActionEncoder(nn.Module):\n",
    "    def __init__(self, num_actions, joint_dim, lantent_dim = 512):\n",
    "        super().__init__()\n",
    "        self.input_dim = joint_dim + num_actions * joint_dim\n",
    "        self.lantent_dim = lantent_dim\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.rand(\n",
    "                1, lantent_dim\n",
    "            )\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=lantent_dim, nhead=2\n",
    "            ),  # joint pos and action seq\n",
    "            num_layers=2,\n",
    "        )\n",
    "        \n",
    "        self.joints_mlp = nn.Linear(joint_dim, lantent_dim)\n",
    "        self.actions_mlp = nn.Linear(joint_dim, lantent_dim)\n",
    "\n",
    "    def forward(self, joints, actions):\n",
    "        joints = self.joints_mlp(joints)\n",
    "        actions = self.actions_mlp(actions)\n",
    "        # print(f'joints: {joints.shape}, actions: {actions.shape}, cls: {self.cls_token.shape}')\n",
    "        X = torch.cat((self.cls_token, joints, actions), dim=0)\n",
    "        # print(f'X: {X.shape}')\n",
    "        X = self.transformer_encoder(X)\n",
    "        return X[0].reshape(-1, self.lantent_dim)\n",
    "\n",
    "\n",
    "model = ActionEncoder(num_actions=4, joint_dim=14, lantent_dim=128)\n",
    "summary(model=model, input_size=[(1, 14), (4, 14)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDecoder(nn.Module):\n",
    "    def __init__(self, lantent_dim, num_actions):\n",
    "        super().__init__()\n",
    "        self.latent_dim = lantent_dim\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.resnet = torchvision.models.resnet18(pretrained=True)\n",
    "        self.resnet.avgpool = nn.Identity()\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        self.joints_mlp = nn.Linear(14, lantent_dim)\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(d_model=lantent_dim, nhead=4),\n",
    "            num_layers=2,\n",
    "        )\n",
    "\n",
    "        def get_sinusoidal_embeddings(num_embeddings, embedding_dim):\n",
    "            position = torch.arange(\n",
    "                0, num_embeddings, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(\n",
    "                0, embedding_dim, 2).float() * (-np.log(10000.0) / embedding_dim))\n",
    "            embeddings = torch.zeros(num_embeddings, embedding_dim)\n",
    "            embeddings[:, 0::2] = torch.sin(position * div_term)\n",
    "            embeddings[:, 1::2] = torch.cos(position * div_term)\n",
    "            return embeddings\n",
    "\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            get_sinusoidal_embeddings(num_actions, lantent_dim), requires_grad=False)\n",
    "\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=nn.TransformerDecoderLayer(d_model=lantent_dim, nhead=4),\n",
    "            num_layers=2,\n",
    "        )\n",
    "        self.mlp = nn.Linear(lantent_dim, 14)\n",
    "\n",
    "    def forward(self, images, z, joint_pos):\n",
    "        # images shape: [4, 3, 480, 640]\n",
    "        features = self.resnet(images)\n",
    "        features = rearrange(features, 'b (d c) -> (b d) c', c=self.latent_dim)\n",
    "        # features shape: [1200, 512]\n",
    "        \n",
    "        joint_pos = self.joints_mlp(joint_pos)\n",
    "\n",
    "        # Concatenate z and joint_pos with features\n",
    "        combined_input = torch.cat((features, z, joint_pos), dim=0)\n",
    "        # combined_input shape: [1202, 512]\n",
    "\n",
    "        # Encode combined input using transformer encoder\n",
    "        encoded_features = self.transformer_encoder(combined_input)\n",
    "        # encoded_features shape: [1202, 512]\n",
    "\n",
    "        # Decode using transformer decoder\n",
    "        tgt = self.position_embeddings\n",
    "        memory = encoded_features\n",
    "        decoded_output = self.transformer_decoder(tgt, memory)\n",
    "        # Map to final output dimension using MLP\n",
    "        output = self.mlp(decoded_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sledge/miniconda3/envs/deep-learning/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sledge/miniconda3/envs/deep-learning/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ActionDecoder                                 [4, 14]                   512\n",
       "├─ResNet: 1-1                                 [4, 153600]               --\n",
       "│    └─Conv2d: 2-1                            [4, 64, 240, 320]         9,408\n",
       "│    └─BatchNorm2d: 2-2                       [4, 64, 240, 320]         128\n",
       "│    └─ReLU: 2-3                              [4, 64, 240, 320]         --\n",
       "│    └─MaxPool2d: 2-4                         [4, 64, 120, 160]         --\n",
       "│    └─Sequential: 2-5                        [4, 64, 120, 160]         --\n",
       "│    │    └─BasicBlock: 3-1                   [4, 64, 120, 160]         73,984\n",
       "│    │    └─BasicBlock: 3-2                   [4, 64, 120, 160]         73,984\n",
       "│    └─Sequential: 2-6                        [4, 128, 60, 80]          --\n",
       "│    │    └─BasicBlock: 3-3                   [4, 128, 60, 80]          230,144\n",
       "│    │    └─BasicBlock: 3-4                   [4, 128, 60, 80]          295,424\n",
       "│    └─Sequential: 2-7                        [4, 256, 30, 40]          --\n",
       "│    │    └─BasicBlock: 3-5                   [4, 256, 30, 40]          919,040\n",
       "│    │    └─BasicBlock: 3-6                   [4, 256, 30, 40]          1,180,672\n",
       "│    └─Sequential: 2-8                        [4, 512, 15, 20]          --\n",
       "│    │    └─BasicBlock: 3-7                   [4, 512, 15, 20]          3,673,088\n",
       "│    │    └─BasicBlock: 3-8                   [4, 512, 15, 20]          4,720,640\n",
       "│    └─Identity: 2-9                          [4, 512, 15, 20]          --\n",
       "│    └─Identity: 2-10                         [4, 153600]               --\n",
       "├─Linear: 1-2                                 [1, 128]                  1,920\n",
       "├─TransformerEncoder: 1-3                     [4802, 128]               --\n",
       "│    └─ModuleList: 2-11                       --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-9      [4802, 128]               593,024\n",
       "│    │    └─TransformerEncoderLayer: 3-10     [4802, 128]               593,024\n",
       "├─TransformerDecoder: 1-4                     [4, 128]                  --\n",
       "│    └─ModuleList: 2-12                       --                        --\n",
       "│    │    └─TransformerDecoderLayer: 3-11     [4, 128]                  659,328\n",
       "│    │    └─TransformerDecoderLayer: 3-12     [4, 128]                  659,328\n",
       "├─Linear: 1-5                                 [4, 14]                   1,806\n",
       "===============================================================================================\n",
       "Total params: 13,685,454\n",
       "Trainable params: 13,684,942\n",
       "Non-trainable params: 512\n",
       "Total mult-adds (G): 49.48\n",
       "===============================================================================================\n",
       "Input size (MB): 14.75\n",
       "Forward/backward pass size (MB): 1160.23\n",
       "Params size (MB): 53.15\n",
       "Estimated Total Size (MB): 1228.13\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder = ActionDecoder(lantent_dim=128, num_actions=4)\n",
    "summary(model=transformer_encoder, input_size=[(4, 3, 480, 640), (1, 128), (1, 14)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = ActionEncoder(num_actions=4, joint_dim=14, lantent_dim=128)\n",
    "        self.mu_phi = nn.Linear(128, 128)\n",
    "        self.logvar = nn.Linear(128, 128)\n",
    "        # output action seq directly\n",
    "        self.decoder = ActionDecoder(lantent_dim=128, num_actions=4)\n",
    "\n",
    "    def encode(self, joints, actions):\n",
    "        z = self.encoder(joints, actions)\n",
    "        return self.mu_phi(z), self.logvar(z)\n",
    "\n",
    "    def reparametrize(self, mu_phi, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std).detach().to(device)\n",
    "        return mu_phi + eps * std\n",
    "\n",
    "    def decode(self, images, z, joint_pos):\n",
    "        return self.decoder(images, z, joint_pos)\n",
    "\n",
    "    def forward(self, current_joints, future_actions, images):\n",
    "        mu_phi, logvar = self.encode(current_joints, future_actions)\n",
    "        z = self.reparametrize(mu_phi, logvar)\n",
    "        return self.decode(images, z, current_joints), mu_phi, logvar\n",
    "\n",
    "    def loss_fn(self, predict, target, mu_phi, logvar):\n",
    "        # KL divergence: Sum over latent dimensions, average over batch\n",
    "        kl_divergence = -0.5 * \\\n",
    "            torch.sum(1 + logvar - mu_phi.pow(2) - logvar.exp(), dim=1).mean()\n",
    "        # Reconstruction loss: Mean squared error\n",
    "        # Sum over features, average over batch\n",
    "        reconstruction = F.mse_loss(predict, target, reduction='mean')\n",
    "\n",
    "        # Total loss\n",
    "        return kl_divergence + reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "CVAE                                               [4, 14]                   --\n",
       "├─ActionEncoder: 1-1                               [1, 128]                  128\n",
       "│    └─Linear: 2-1                                 [1, 128]                  1,920\n",
       "│    └─Linear: 2-2                                 [4, 128]                  1,920\n",
       "│    └─TransformerEncoder: 2-3                     [6, 128]                  --\n",
       "│    │    └─ModuleList: 3-1                        --                        1,186,048\n",
       "├─Linear: 1-2                                      [1, 128]                  16,512\n",
       "├─Linear: 1-3                                      [1, 128]                  16,512\n",
       "├─ActionDecoder: 1-4                               [4, 14]                   512\n",
       "│    └─ResNet: 2-4                                 [4, 153600]               --\n",
       "│    │    └─Conv2d: 3-2                            [4, 64, 240, 320]         9,408\n",
       "│    │    └─BatchNorm2d: 3-3                       [4, 64, 240, 320]         128\n",
       "│    │    └─ReLU: 3-4                              [4, 64, 240, 320]         --\n",
       "│    │    └─MaxPool2d: 3-5                         [4, 64, 120, 160]         --\n",
       "│    │    └─Sequential: 3-6                        [4, 64, 120, 160]         147,968\n",
       "│    │    └─Sequential: 3-7                        [4, 128, 60, 80]          525,568\n",
       "│    │    └─Sequential: 3-8                        [4, 256, 30, 40]          2,099,712\n",
       "│    │    └─Sequential: 3-9                        [4, 512, 15, 20]          8,393,728\n",
       "│    │    └─Identity: 3-10                         [4, 512, 15, 20]          --\n",
       "│    │    └─Identity: 3-11                         [4, 153600]               --\n",
       "│    └─Linear: 2-5                                 [1, 128]                  1,920\n",
       "│    └─TransformerEncoder: 2-6                     [4802, 128]               --\n",
       "│    │    └─ModuleList: 3-12                       --                        1,186,048\n",
       "│    └─TransformerDecoder: 2-7                     [4, 128]                  --\n",
       "│    │    └─ModuleList: 3-13                       --                        1,318,656\n",
       "│    └─Linear: 2-8                                 [4, 14]                   1,806\n",
       "====================================================================================================\n",
       "Total params: 14,908,494\n",
       "Trainable params: 14,907,982\n",
       "Non-trainable params: 512\n",
       "Total mult-adds (G): 49.49\n",
       "====================================================================================================\n",
       "Input size (MB): 14.75\n",
       "Forward/backward pass size (MB): 1160.47\n",
       "Params size (MB): 57.52\n",
       "Estimated Total Size (MB): 1232.73\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CVAE()\n",
    "summary(model=model, input_size=[(1, 14), (4, 14), (4, 3, 480, 640)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
